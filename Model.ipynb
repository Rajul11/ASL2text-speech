{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAwEFnCUu2fz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapipe as mp\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hand Detection**"
      ],
      "metadata": {
        "id": "en6S20pUvAEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the Model\n",
        "mpHands = mp.solutions.hands\n",
        "hands = mpHands.Hands(\n",
        "\tstatic_image_mode=False,\n",
        "\tmodel_complexity=1,\n",
        "\tmin_detection_confidence=0.75,\n",
        "\tmin_tracking_confidence=0.75,\n",
        "\tmax_num_hands=2)"
      ],
      "metadata": {
        "id": "x9D6B6navHbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**data loading and transforming**"
      ],
      "metadata": {
        "id": "sM295t26vSAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "data_dir = 'D:/MIU/Courses/06 - ML/ASL/Custom_Dataset/latest_full'\n",
        "\n",
        "# Collect all image file paths and their corresponding class labels\n",
        "file_paths = []\n",
        "labels = []\n",
        "\n",
        "for class_dir in os.listdir(data_dir):\n",
        "    class_path = os.path.join(data_dir, class_dir)\n",
        "    if os.path.isdir(class_path):\n",
        "        for img_file in os.listdir(class_path):\n",
        "            file_paths.append(os.path.join(class_path, img_file))\n",
        "            labels.append(class_dir)\n",
        "\n",
        "file_paths = np.array(file_paths)\n",
        "labels = np.array(labels)\n",
        "class_names = np.unique(labels)\n",
        "\n",
        "# Function to load images into memory\n",
        "def load_images(file_paths):\n",
        "    images = []\n",
        "    valid_labels = []\n",
        "    count = 0\n",
        "    for file_path, label in zip(file_paths, labels):\n",
        "\n",
        "        img = cv2.imread(file_path)\n",
        "        # Convert BGR image to RGB image\n",
        "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        # Process the RGB image\n",
        "        results = hands.process(imgRGB)\n",
        "        if results.multi_hand_landmarks:\n",
        "            #Used for data normalization\n",
        "            x_min = min([landmark.x for landmark in results.multi_hand_landmarks[0].landmark])\n",
        "            y_min = min([landmark.y for landmark in results.multi_hand_landmarks[0].landmark])\n",
        "            x_max = max([landmark.x for landmark in results.multi_hand_landmarks[0].landmark])\n",
        "            y_max = max([landmark.y for landmark in results.multi_hand_landmarks[0].landmark])\n",
        "            w = x_max - x_min\n",
        "            h = y_max - y_min\n",
        "            #Generate normlized data List with its flipped version, and flatten\n",
        "            landmarks      = [(   (landmark.x - x_min) / w , (landmark.y - y_min) / h ) for landmark in results.multi_hand_landmarks[0].landmark]\n",
        "            landmarks_Flip = [(1-((landmark.x - x_min) / w), (landmark.y - y_min) / h ) for landmark in results.multi_hand_landmarks[0].landmark]\n",
        "            landmarks = list(sum(landmarks, ()))\n",
        "            landmarks_Flip = list(sum(landmarks_Flip, ()))\n",
        "            #Append to the processed dataset. X(images), y(valid_labels)\n",
        "            images.append(landmarks)\n",
        "            valid_labels.append(label)\n",
        "            images.append(landmarks_Flip)\n",
        "            valid_labels.append(label)\n",
        "        else:\n",
        "            count += 1\n",
        "\n",
        "\n",
        "    print(\"Missed = {}\".format(count))\n",
        "    return np.array(images), np.array(valid_labels)\n",
        "# Load all images\n",
        "images, valid_labels = load_images(file_paths)\n",
        "\n",
        "# Create stratified train/test split\n",
        "train_files, test_files, train_labels, test_labels = train_test_split(\n",
        "    images, valid_labels, test_size=0.3, stratify=valid_labels, random_state=42\n",
        ")\n",
        "# Further split the test set into validation and test sets\n",
        "val_files, test_files, val_labels, test_labels = train_test_split(\n",
        "    test_files, test_labels, test_size=0.5, stratify=test_labels, random_state=42\n",
        ")\n",
        "encoder = OneHotEncoder(categories='auto', sparse=False)\n",
        "# Reshape labels for OneHotEncoder\n",
        "train_labels_reshaped = train_labels.reshape(-1, 1)\n",
        "val_labels_reshaped = val_labels.reshape(-1, 1)\n",
        "test_labels_reshaped = test_labels.reshape(-1, 1)\n",
        "# Fit and transform labels\n",
        "train_labels_encoded = encoder.fit_transform(train_labels_reshaped)\n",
        "val_labels_encoded = encoder.transform(val_labels_reshaped)\n",
        "test_labels_encoded = encoder.transform(test_labels_reshaped)\n",
        "\n",
        "# Check the shapes of the datasets\n",
        "print(f\"Train files shape: {train_files.shape}\")\n",
        "print(f\"Train labels shape: {train_labels_encoded.shape}\")\n",
        "print(f\"Validation files shape: {val_files.shape}\")\n",
        "print(f\"Validation labels shape: {val_labels_encoded.shape}\")\n",
        "print(f\"Test files shape: {test_files.shape}\")\n",
        "print(f\"Test labels shape: {test_labels_encoded.shape}\")\n"
      ],
      "metadata": {
        "id": "5myP3H50vfG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**modeling**"
      ],
      "metadata": {
        "id": "i2DkMsgRvkh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple CNN model\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=42, activation='relu'))  # First hidden layer with 64 neurons\n",
        "model.add(Dropout(0.5))  # Dropout to prevent overfitting\n",
        "model.add(Dense(128, activation='relu'))  # Second hidden layer with 128 neurons\n",
        "model.add(Dropout(0.5))  # Dropout to prevent overfitting\n",
        "model.add(Dense(128, activation='relu'))  # Second hidden layer with 128 neurons\n",
        "model.add(Dropout(0.5))  # Dropout to prevent overfitting\n",
        "model.add(Dense(26, activation='softmax'))  # Output layer\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "checkpoint = ModelCheckpoint('D:/MIU/Courses/06 - ML/ASL_Model_V4/model.keras', save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=3, min_lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "#steps_per_epoch = len(train_files) // train_generator.batch_size\n",
        "#validation_steps = len(val_files) // validation_generator.batch_size\n",
        "\n",
        "history = model.fit(\n",
        "    train_files, train_labels_encoded,\n",
        "    batch_size = 64,\n",
        "    #steps_per_epoch=steps_per_epoch,\n",
        "    epochs=100,\n",
        "    validation_data=(val_files, val_labels_encoded),\n",
        "    #validation_steps=validation_steps,\n",
        "    callbacks=[checkpoint, early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "# Evaluate on test data\n",
        "test_loss, test_acc = model.evaluate(test_files, test_labels_encoded )\n",
        "print(f'Test accuracy: {test_acc}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MQkEyIaxvznD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "exofUTo_v12i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "ZStP5Dl_wDWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**"
      ],
      "metadata": {
        "id": "acR8Ub1NwGsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "y_pred = model.predict(test_files)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(test_labels_encoded, axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "X4ltIX3rwNVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted')\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1_score = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1_score)\n",
        "\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "GI7wBtZMwSj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.categories_"
      ],
      "metadata": {
        "id": "sIaVtQT7wcmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "ax = plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax=ax)  # Annotate cells with values\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'])\n",
        "ax.yaxis.set_ticklabels(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DlqEPJJuwfcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "USE MODEL WITH WEBCAM"
      ],
      "metadata": {
        "id": "WgYJMgl8wlEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subframing (for hand detection box visualization)"
      ],
      "metadata": {
        "id": "bZmqb-dXwpiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bounding_box(landmarks, image_width, image_height, scale=1.2):\n",
        "    x_coords = [landmark.x * image_width for landmark in landmarks.landmark]\n",
        "    y_coords = [landmark.y * image_height for landmark in landmarks.landmark]\n",
        "    x_min, x_max = int(min(x_coords)), int(max(x_coords))\n",
        "    y_min, y_max = int(min(y_coords)), int(max(y_coords))\n",
        "\n",
        "    # Calculate the center of the bounding box\n",
        "    x_center = (x_min + x_max) // 2\n",
        "    y_center = (y_min + y_max) // 2\n",
        "\n",
        "    # Calculate the size of the bounding box\n",
        "    box_size = max(x_max - x_min, y_max - y_min) * scale\n",
        "\n",
        "    # Ensure the bounding box is a square\n",
        "    half_size = int(box_size // 2)\n",
        "\n",
        "    # Calculate new min and max coordinates\n",
        "    x_min_new = max(x_center - half_size, 0)\n",
        "    x_max_new = min(x_center + half_size, image_width)\n",
        "    y_min_new = max(y_center - half_size, 0)\n",
        "    y_max_new = min(y_center + half_size, image_height)\n",
        "\n",
        "    return x_min_new, y_min_new, x_max_new, y_max_new\n"
      ],
      "metadata": {
        "id": "j1Jl9m4Iw0T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For model's output-sequence processing**"
      ],
      "metadata": {
        "id": "C_72Pc5Pw5tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from textblob import TextBlob\n",
        "\n",
        "smoothing_window_size = 5\n",
        "autocorrection_threshold = 3\n",
        "\n",
        "def smooth_predictions(predictions, window_size = smoothing_window_size):\n",
        "    smoothed = []\n",
        "    for i in range(len(predictions) - window_size + 1):\n",
        "        window = predictions[i:i + window_size]\n",
        "        most_common = Counter(window).most_common(1)[0][0]\n",
        "        smoothed.append(most_common)\n",
        "    return smoothed\n",
        "\n",
        "def remove_redundant(predictions, threshold=4):\n",
        "    filtered = []\n",
        "    last_char = predictions[0]\n",
        "    count = 0\n",
        "\n",
        "    for char in predictions:\n",
        "        if char == last_char:\n",
        "            count += 1\n",
        "        else:\n",
        "            if count >= threshold:\n",
        "                filtered.append(last_char)\n",
        "            count = 1\n",
        "            last_char = char\n",
        "    if count >= threshold:\n",
        "            filtered.append(last_char)\n",
        "\n",
        "    return filtered\n",
        "\n",
        "def process_predicted_word(letters_list):\n",
        "    letters_list = letters_list\n",
        "    #if input is too small -> don't do processing\n",
        "    if len(letters_list) < smoothing_window_size:\n",
        "        return ''.join(letters_list).lower()\n",
        "    else:\n",
        "        smoothing = smooth_predictions(letters_list)\n",
        "        filter_redundants= remove_redundant(smoothing)\n",
        "        if len(filter_redundants) <= autocorrection_threshold:\n",
        "            return ''.join(filter_redundants).lower()\n",
        "        autocorrected = str(TextBlob(''.join(filter_redundants).lower()).correct())\n",
        "        return autocorrected\n",
        "\n",
        "\n",
        "#FOR DEMONSTRATION\n",
        "\n",
        "x = ['h','h','h','h','h','x','a','a','a','a','x','x','a','a','a','p','p','p','p','p','y','y','y','y','y','y']\n",
        "print(\"input: \",x)\n",
        "print(\"input flatten: \",''.join(x))\n",
        "x = np.array(smooth_predictions(x))\n",
        "#smooth_predictions(x.tolist())\n",
        "print(\"Smoothing: \",''.join(x))\n",
        "x = np.array(remove_redundant(x))\n",
        "print(\"Removing Redundants: \",''.join(x))\n",
        "x = str(TextBlob(''.join(x)).correct())\n",
        "print(\"Autocorrecting: \",x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T1pFOcv2w8Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running the Model**"
      ],
      "metadata": {
        "id": "_whXIVGixKgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import threading\n",
        "\n",
        "#For Adjusment\n",
        "prediction_threshold = 0.3\n",
        "wait_between_words = 3 #seconds\n",
        "\n",
        "# Start capturing video from webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "#Words producing control\n",
        "word = []\n",
        "produced_words = []\n",
        "last_detection_time = datetime.now()\n",
        "waiting_input = True\n",
        "\n",
        "while True:\n",
        "    #to calculate FPS later\n",
        "    start_time = datetime.now()\n",
        "    # Read video frame by frame\n",
        "    success, img = cap.read()\n",
        "\n",
        "    # Flip the image(frame)\n",
        "    img = cv2.flip(img, 1)\n",
        "\n",
        "    # Convert BGR image to RGB image\n",
        "    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process the RGB image\n",
        "    results = hands.process(imgRGB)\n",
        "\n",
        "    # If hands are present in image\n",
        "    if results.multi_hand_landmarks:\n",
        "        #Words producing control\n",
        "        last_detection_time = datetime.now()\n",
        "        waiting_input = False\n",
        "\n",
        "        for hand_landmarks in results.multi_hand_landmarks:\n",
        "            # Get bounding box\n",
        "            h, w, _ = img.shape\n",
        "            x_min, y_min, x_max, y_max = get_bounding_box(hand_landmarks, w, h)\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=(255,0,0), thickness=2)\n",
        "\n",
        "        #Normalize landmarks\n",
        "        x_min = min([landmark.x for landmark in results.multi_hand_landmarks[0].landmark])\n",
        "        y_min = min([landmark.y for landmark in results.multi_hand_landmarks[0].landmark])\n",
        "        x_max = max([landmark.x for landmark in results.multi_hand_landmarks[0].landmark])\n",
        "        y_max = max([landmark.y for landmark in results.multi_hand_landmarks[0].landmark])\n",
        "        w = x_max - x_min\n",
        "        h = y_max - y_min\n",
        "        landmarks = [( (landmark.x - x_min) / w, (landmark.y - y_min) / h ) for landmark in results.multi_hand_landmarks[0].landmark]\n",
        "        landmarks = list(sum(landmarks, ()))\n",
        "\n",
        "        #To Show Coordinates (FOR DEVELOPING)\n",
        "        '''\n",
        "        formatted_list = [f\"{num:.{2}f}\" for num in landmarks]\n",
        "        result_string1 = \", \".join(formatted_list[0:10])\n",
        "        result_string2 = \", \".join(formatted_list[10:20])\n",
        "        result_string3 = \", \".join(formatted_list[20:30])\n",
        "        result_string4 = \", \".join(formatted_list[30:-1])\n",
        "        cv2.putText(img, result_string1, (10, 200), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 255, 0), 2)\n",
        "        cv2.putText(img, result_string2, (10, 250), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 255, 0), 2)\n",
        "        cv2.putText(img, result_string3, (10, 300), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 255, 0), 2)\n",
        "        cv2.putText(img, result_string4, (10, 350), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 255, 0), 2)\n",
        "        '''\n",
        "\n",
        "        #Make Prediction\n",
        "        landmarks = np.array(landmarks).reshape(1, -1)\n",
        "        y = model.predict(landmarks)\n",
        "\n",
        "        #Set Confidence Threshold (only proceede to predict when meeting minimum threshold)\n",
        "        if max(y[0]) > prediction_threshold:\n",
        "            #Decoding Predicion\n",
        "            y_decoded =  encoder.inverse_transform(y)[0][0]\n",
        "            cv2.putText(img, y_decoded, (250, 50), cv2.FONT_HERSHEY_COMPLEX, 0.9, (0, 255, 0), 2)\n",
        "            cv2.putText(img, \"P: {:.2f}\".format(max(y[0])), (300, 50), cv2.FONT_HERSHEY_COMPLEX, 0.9, (0, 255, 0), 2)\n",
        "            word.append(y_decoded)\n",
        "            #Show Prediction Distribution in Frame (FOR DEVELOPING)\n",
        "            '''\n",
        "            string_value_list = [str(v) for v in np.around(y[0], decimals=2)]\n",
        "            string_value_list1 = ','.join(string_value_list[0:13])\n",
        "            string_value_list2 = ','.join(string_value_list[13:-1])\n",
        "            cv2.putText(img, string_value_list1, (50, 100), cv2.FONT_HERSHEY_COMPLEX, 0.7, (0, 255, 0), 1)\n",
        "            cv2.putText(img, string_value_list2, (50, 150), cv2.FONT_HERSHEY_COMPLEX, 0.7, (0, 255, 0), 1)\n",
        "            '''\n",
        "\n",
        "    else: #NO HAND IN FRAME -> Words producing control\n",
        "        if (datetime.now() - last_detection_time).total_seconds() > wait_between_words and waiting_input == False:\n",
        "            #PRODUCE WORD\n",
        "            processed_word = process_predicted_word(word)\n",
        "            produced_words.append(processed_word)\n",
        "            #threading.Thread(target=text_to_speech, args=(processed_word,)).start()\n",
        "            word = []\n",
        "            waiting_input = True\n",
        "        elif waiting_input == False:\n",
        "            cv2.putText(img, str(round((datetime.now() - last_detection_time).total_seconds(),)), (10, 350), cv2.FONT_HERSHEY_COMPLEX, 0.9, (0, 255, 0), 2)\n",
        "    #Measure FPS and Show it\n",
        "    end_time = datetime.now()\n",
        "    time_difference = (end_time - start_time).total_seconds()\n",
        "    cv2.putText(img, \"FPS: {}\".format(round(1/time_difference)), (10, 450), cv2.FONT_HERSHEY_COMPLEX, 0.9, (0, 255, 0), 2)\n",
        "    #Print Last word\n",
        "    cv2.putText(img, ' '.join(produced_words), (10, 400), cv2.FONT_HERSHEY_COMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "\t# Display Video and when 'q' is entered, destroy the window\n",
        "    cv2.imshow('Image', img)\n",
        "    if cv2.waitKey(1) & 0xff == ord('q'):\n",
        "        break\n"
      ],
      "metadata": {
        "id": "SrCOvmfVxQB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving Pickle**"
      ],
      "metadata": {
        "id": "bhZwjbZUxVqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Save the model architecture to a JSON string\n",
        "model_json = model.to_json()\n",
        "\n",
        "# Save the model weights to a numpy array\n",
        "model_weights = model.get_weights()\n",
        "\n",
        "# Create a dictionary to store the model architecture and weights\n",
        "model_dict = {\n",
        "    'model_json': model_json,\n",
        "    'model_weights': model_weights\n",
        "}\n",
        "\n",
        "# Save the dictionary to a pickle file\n",
        "with open('D:/MIU/Courses/06 - ML/ASL_Model_V4/model.pkl', 'wb') as f:\n",
        "    pickle.dump(model_dict, f)\n"
      ],
      "metadata": {
        "id": "eIJAdKQixalI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Pickle**"
      ],
      "metadata": {
        "id": "T298lnh_xe0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import model_from_json\n",
        "import pickle\n",
        "\n",
        "# Load the dictionary from the pickle file\n",
        "with open('D:/MIU/Courses/06 - ML/ASL_Model2_Dawit/model.pkl', 'rb') as f:\n",
        "    model_dict = pickle.load(f)\n",
        "\n",
        "# Recreate the model architecture from the JSON string\n",
        "model_json = model_dict['model_json']\n",
        "model = model_from_json(model_json)\n",
        "\n",
        "# Load the model weights\n",
        "model_weights = model_dict['model_weights']\n",
        "model.set_weights(model_weights)\n",
        "\n",
        "# The model is now loaded and ready to use\n"
      ],
      "metadata": {
        "id": "SSqgmQpOxjYK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}